A rule-based program, performing lexical tokenization, is called tokenizer, or scanner,
although scanner is also a term for the first stage of a lexer.
A lexer forms the first phase of a compiler frontend in processing. 
