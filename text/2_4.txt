Lexical tokenization is conversion of a text into (semantically or syntactically)
meaningful lexical tokens belonging to categories defined by a "lexer" program. 
In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc.