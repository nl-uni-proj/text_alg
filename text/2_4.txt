Lexical tokenization is conversion of a text into meaningful lexical tokens
belonging to categories defined by a "lexer" program.
In case of a natural language, those categories
include nouns, verbs, adjectives, punctuations etc.